{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "name: GDPTools Conus404-daily Aggregation\n",
        "creator: Richard McDonald USGS IMPD\n",
        "title: 'GDPTools: Conus404-daily Aggregation'\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the use of [GDPTools](https://gdptools.readthedocs.io/en/develop/index.html) for spatially interpolating (Aggregation) Conus404 data to a set of HUC12s in the Delaware River Basin.\n",
        "\n",
        "## Summary\n",
        "This use-case demonstrates a common spatial interpolation pattern in hydrologic sciences.  Interpolating gridded data, in this case the Conus404 dataset, to Polygons representing basins, such as HUC12s.  GDPTools is a pyhton package for Geospatial Interpolation, developed by the USGS NHGF project.  We use GDPTools to generate a weights file for area-weighted spatial interpolation, interpolating the time-dependend gridded data to the HUC12s. The [Conus404](https://www.sciencebase.gov/catalog/item/6372cd09d34ed907bf6c6ab1) data release describes Conus404 as \"... a unique, high-resolution hydro-climate dataset appropriate for forcing hydrological models and conducting meteorological analysis over the contiguous United States\".  This use-case uses the HyTest intake catalog which provides a convenient method to access the Conus404 data.  \n",
        "\n",
        "## Open Source python Tools\n",
        "- gdptools: GeoDataProcessing Tools (GDPTools) is a python package for geospatial interpolation\n",
        "- [HyTest intake catalog](https://github.com/hytest-org/hytest/tree/main/dataset_catalog)\n",
        "- [NHGF data service]( https://labs.waterdata.ugsgs.gov) clients:\n",
        "  - We use the collection of [HyRiver](https://docs.hyriver.io/examples/notebooks/nhdplus.html) Packages for accessing a set of HUC12 polygons in the Delaware River Basin programatically using the [NLDI](https://docs.hyriver.io/readme/pynhd.html) and [WaterData](https://docs.hyriver.io/readme/pynhd.html)\n",
        "\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "# Common python packages\n",
        "import xarray as xr\n",
        "import hvplot.xarray\n",
        "import hvplot.pandas\n",
        "import hvplot.dask\n",
        "import intake\n",
        "import warnings\n",
        "import intake_xarray\n",
        "import datetime\n",
        "import holoviews as hv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "# HyRiver packages\n",
        "from pynhd import NLDI, WaterData\n",
        "\n",
        "# GDPTools packages\n",
        "from gdptools import AggGen, UserCatData, WeightGen\n",
        "\n",
        "\n",
        "hv.extension(\"bokeh\")\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GDPTools version\n",
        "Note this use-case is currently using the following version of GDPTools.  GDPTools is in an active state of development, and if the viewer of this use-case would like to use GDPTools for their work, it would be worth checking if you have the most up-to-date version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "!conda list gdptools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HyRiver Packages\n",
        "Here we use two HyRiver clients, `NLDI` and `WaterData1` to:\n",
        "- Define the upstream basin at a USGS gage towards the downstream reach of the Delaware River\n",
        "- Use the basin geometry to extract HUC21s within the geometry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "# USGS gage 01482100 Delaware River at Del Mem Bridge at Wilmington De\n",
        "gage_id = '01482100'\n",
        "nldi = NLDI()\n",
        "del_basins = nldi.get_basins(gage_id)\n",
        "huc12_basins = WaterData('huc12').bygeom(del_basins.geometry[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View the content of the resulting GeoDataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "huc12_basins.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note the crs associate with the GeoDataFrame\n",
        "This value will be used later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "huc12_basins.crs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reduce the context of the GeoDataFrame to only the columns necessary for the analysis\n",
        "GDPTools requires only the column used to identify the basins and the geometry.  To reduce the memory overhead of the calculations below, it's a good practice to remove unneccessary columns.  First we list the columns and then we keep only `huc12` and `geometry`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "print(huc12_basins.columns)\n",
        "working_gdf = huc12_basins[[\"huc12\", \"geometry\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot the resulting GeoDataFrame\n",
        "\n",
        "\n",
        "\n",
        "## HyTest Intake Catalog and Conus404\n",
        "Further examples of using the HyTest catalog can be found [here](https://github.com/hytest-org/hytest/tree/main/dataset_catalog).  In the following cells we step into the catalog to retrieve the `conus404-daily-osn`.  The data with \"-osn\" extensions can be accessed without authentication.\n",
        "\n",
        "### Open the catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "# open the hytest data intake catalog\n",
        "hytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
        "list(hytest_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Open the conus404 catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "# open the conus404 sub-catalog\n",
        "cat = hytest_cat['conus404-catalog']\n",
        "list(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Open the daily data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "## Select the dataset you want to read into your notebook and preview its metadata\n",
        "dataset = 'conus404-daily-osn' \n",
        "cat[dataset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load conus404 daily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "print(f\"Reading {dataset} metadata...\", end='')\n",
        "ds = cat[dataset].to_dask().metpy.parse_cf()\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View the xarray representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conus404 projection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "c404_proj = \"+proj=lcc +lat_0=39.1000061035156 +lon_0=-97.9000015258789 +lat_1=30 +lat_2=50 +x_0=0 +y_0=0 +R=6370000 +units=m +no_defs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GDPTools\n",
        "In the following cells GDPtools if first used to generate a weighting file used for area-weighted statistics and secondly to apply the weighting file to interpolate the Conus404 gridded data to each of the HUC12 polygons using an area-weighted mean.  GDPTools has a number of data classes to parameterize the input data, which can be though of as the source and target data with some accompanying attributes of each.  In this case we use the `UserCatData` class.\n",
        "\n",
        "Source Data:\n",
        "- ds: Xarray dataset\n",
        "- x_coord: Name of x-coordinate\n",
        "- y_cord: Name of y-coordinate\n",
        "- t_coord: Name of t-coordinate\n",
        "- proj_ds: The source data's projection.  This can be anything that can by read by [pyproj.crs.CRS.from_user_input()](https://pyproj4.github.io/pyproj/stable/api/crs/crs.html#pyproj.crs.CRS.from_user_input) method.\n",
        "- var: List of variables in the xarray dataset that will be processed.\n",
        "\n",
        "Target Data:\n",
        "- f_feature: The GeoDataFrame - In this case `working_gdf` define above.\n",
        "- proj_feature: Similar to proj_ds above but for the Target GeoDataFrame.\n",
        "- id_feature: The column heading of the feature used to identify the target for the interpolation - In this case \"huc12\"\n",
        "\n",
        "Period:\n",
        "The period of interest.  This is where things can go sideways.  It's always a good strategy to start with a small slice of data and work up.  For large Target datasets, for example HUC12s for all of CONUS, you would quickly run out of memory if processing more that a year at a time.  Also some servers have a limit on the size of the data requested.  In this case we have a small example - so it's no issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "data_crs = c404_proj\n",
        "x_coord = \"x\"\n",
        "y_coord = \"y\"\n",
        "t_coord = \"time\"\n",
        "sdate = \"1979-10-01T00:00:00.000000000\"\n",
        "edate = \"1979-10-07T00:00:00.000000000\"\n",
        "var = [\"T2\", \"ACRAINLSM\"]\n",
        "shp_crs = 4326\n",
        "shp_poly_idx = \"huc12\"\n",
        "wght_gen_crs = 6931\n",
        "\n",
        "user_data = UserCatData(\n",
        "    ds=ds,\n",
        "    proj_ds=data_crs,\n",
        "    x_coord=x_coord,\n",
        "    y_coord=y_coord,\n",
        "    t_coord=t_coord,\n",
        "    var=var,\n",
        "    f_feature=working_gdf,\n",
        "    proj_feature=shp_crs,\n",
        "    id_feature=shp_poly_idx,\n",
        "    period=[sdate, edate],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate the weights\n",
        "Generate the weights and save to a file for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "genwghts = True\n",
        "wght_gen = WeightGen(\n",
        "    user_data=user_data,\n",
        "    method=\"serial\",\n",
        "    output_file=\"wghts_drb_ser_c404daily.csv\",\n",
        "    weight_gen_crs=6931\n",
        ")\n",
        "\n",
        "if genwghts:\n",
        "    wdf = wght_gen.calculate_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aggregate the data using the weights generated above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "agg_gen = AggGen(\n",
        "    user_data=user_data,\n",
        "    stat_method=\"masked_mean\",\n",
        "    agg_engine=\"serial\",\n",
        "    agg_writer=\"netcdf\",\n",
        "    weights='wghts_drb_ser_c404daily.csv',\n",
        "    out_path='.',\n",
        "    file_prefix=\"testing_p\"\n",
        ")\n",
        "ngdf, ds_out = agg_gen.calculate_agg()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a plot of the resulting interpolatoin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "aggdata = agg_gen.agg_data.get('T2').da\n",
        "aggdata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "wvar = 'T2'\n",
        "time_step = \"1979-10-03T00:00:00.000000000\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "vmin = np.nanmin(aggdata.sel(time=time_step).values)\n",
        "vmax = np.nanmax(aggdata.sel(time=time_step).values)\n",
        "print(vmin, vmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "source": [
        "ngdf['T2'] = ds_out[wvar].sel(time=time_step)\n",
        "c404_agg = ngdf.to_crs(4326).hvplot(\n",
        "    c='T2', geo=True, coastline='50m', frame_width=300, alpha=1, cmap='viridis', clim=(vmin, vmax),\n",
        "    xlabel=\"longitude\", ylabel=\"latitude\", clabel=\"monthly water evaporation (mm)\", xlim=(-76.8, -73.8),\n",
        "    title=\"DRB HUC12 area-weighted average\", aspect='equal'\n",
        ")\n",
        "c404_raw = aggdata.sel(time=time_step).hvplot.quadmesh(\n",
        "    'lon', 'lat', 'T2', cmap='viridis', alpha=1, grid=True, geo=True, coastline='50m', frame_width=300, clim=(vmin,vmax),\n",
        "    xlabel=\"longitude\", ylabel=\"latitude\", clabel=\"monthly water evaporation (mm)\", xlim=(-76.8, -73.8),\n",
        "    title=\"TerraClimate Monthly AET\", aspect='equal'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}